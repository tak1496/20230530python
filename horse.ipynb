{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e636a46-c0a5-4856-9d9d-473db3b570b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Downloading tensorflow_intel-2.12.0-cp310-cp310-win_amd64.whl (272.8 MB)\n",
      "     -------------------------------------- 272.8/272.8 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.10.tar.gz (1.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.2-cp310-abi3-win_amd64.whl (422 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.2-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.6.3)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.4.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-win_amd64.whl (120 kB)\n",
      "     -------------------------------------- 120.4/120.4 kB 7.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.14)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.10-py3-none-any.whl size=1480617 sha256=054c17a96a4abe782a4e681b16855301e7f9c4e858f4db7b80102cd7d9f3ed36\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\f8\\55\\5b\\9dde9a2af48db48d64b8cc3877f0670cf11c5d78de392c3f05\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 jax-0.4.10 keras-2.12.0 libclang-16.0.0 ml-dtypes-0.1.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.2 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24324\\993737537.py:50: FutureWarning: pivot_table dropped a column because it failed to aggregate. This behavior is deprecated and will raise in a future version of pandas. Select only the columns that can be aggregated.\n",
      "  table_jockey = pd.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "df = pd.read_csv('horsedata.csv', encoding='shift-jis')\n",
    "\n",
    "# 前走成績\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "df['days'] = pd.to_datetime(df['days'])\n",
    "name_days_df = df[[\"horsename\", \"days\", \"pop\",\n",
    "                   \"odds\", \"rank3\", \"rank4\", \"3ftime\", \"result\"]].sort_values(['horsename', 'days'])\n",
    "\n",
    "name_list = name_days_df['horsename'].unique()\n",
    "df_list = []\n",
    "\n",
    "for name in name_list:\n",
    "    name_df = name_days_df[name_days_df['horsename'] == name]\n",
    "    shift_name_df = name_df[[\"pop\", \"odds\", \"rank3\",\n",
    "                             \"rank4\", \"3ftime\", \"result\"]].shift(1)\n",
    "    shift_name_df['horsename'] = name\n",
    "    df_list.append(shift_name_df)\n",
    "\n",
    "df_before = pd.concat(df_list)\n",
    "df_before['days'] = name_days_df['days']\n",
    "\n",
    "df_before = df_before.rename(columns={'pop': 'pre_pop', 'odds': 'pre_odds', 'rank3': 'pre_rank3',\n",
    "                                      'rank4': 'pre_rank4', '3ftime': 'pre_3ftime', 'result': 'pre_result'})\n",
    "\n",
    "df = pd.merge(df, df_before, on=['horsename', 'days'], how='inner')\n",
    "\n",
    "\n",
    "# 騎手のコースごとの連対率\n",
    "df.loc[df['result'] >= 3, 'result'] = 0\n",
    "df.loc[df['result'] == 2, 'result'] = 1\n",
    "\n",
    "table_jockey = pd.pivot_table(\n",
    "    df, index='jocky', columns='place', values='result', aggfunc='mean', dropna=False)\n",
    "table_jockey = table_jockey.fillna(0)\n",
    "\n",
    "table_jockey = pd.DataFrame(table_jockey)\n",
    "table_jockey = table_jockey.round(4)\n",
    "table_jockey = table_jockey.add_prefix('jockey_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27989ed-a297-4bfe-8796-326e97d2a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24324\\4016950798.py:31: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  time_3f = df.groupby(index).mean()['3ftime']\n"
     ]
    }
   ],
   "source": [
    "#父親のコース、距離、馬場の連対率と脚質\n",
    "\n",
    "df.loc[df['result'] >= 3, 'result'] = 0\n",
    "df.loc[df['result'] == 2, 'result'] = 1\n",
    "\n",
    "index = 'father'\n",
    "\n",
    "table_father_place = pd.pivot_table(df, index=index, columns='place', values='result', aggfunc='mean',\n",
    "                                    dropna=False)\n",
    "table_father_distance = pd.pivot_table(df, index=index, columns='distance', values='result', aggfunc='mean',\n",
    "                                       dropna=False)\n",
    "table_father_turf = pd.pivot_table(df, index=index, columns='turf', values='result', aggfunc='mean',\n",
    "                                   dropna=False)\n",
    "table_father_condition = pd.pivot_table(df, index=index, columns='condition', values='result', aggfunc='mean',\n",
    "                                        dropna=False)\n",
    "\n",
    "table_father = pd.merge(table_father_place, table_father_distance, on=index, how='left')\n",
    "table_father = pd.merge(table_father, table_father_turf, on=index, how='left')\n",
    "table_father = pd.merge(table_father, table_father_condition, on=index, how='left')\n",
    "\n",
    "table_father1 = table_father.fillna(0)\n",
    "\n",
    "df['legtype'] = df['legtype'].map({'逃げ': 0, '先行': 1, '差し': 2, '追込': 3, '自在': 4})\n",
    "legtypes = df.groupby(index).legtype.apply(lambda x: x.mode()).reset_index()\n",
    "\n",
    "legtype = pd.DataFrame(legtypes)\n",
    "legtype['legtype'] = legtype['legtype'].map({0: '逃げ', 1: '先行', 2: '差し', 3: '追込', 4: '自在'})\n",
    "\n",
    "legtype = legtype.drop('level_1', axis=1)\n",
    "\n",
    "time_3f = df.groupby(index).mean()['3ftime']\n",
    "time3f = pd.DataFrame(time_3f)\n",
    "\n",
    "father = pd.merge(table_father1, legtype, on=index, how='left')\n",
    "father = pd.merge(father, time3f, on=index, how='left')\n",
    "\n",
    "father = father.round(3)\n",
    "father = father.add_prefix('{}_'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed715ee-bb0c-4bf1-9cce-5af67e81be1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'father_3ftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'father_3ftime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre_3_to_4time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_rank4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_rank3\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre_3_to_4time_hi*2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_rank4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_rank3\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfather_3f_to_my\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfather_3ftime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_3ftime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfathertype_3f_to_my\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfathertype_3ftime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_3ftime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre_pop_now_pop\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_pop\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'father_3ftime'"
     ]
    }
   ],
   "source": [
    "#特徴量の生成\n",
    "\n",
    "d_ranking = lambda x: 1 if x in [1, 2] else 0\n",
    "df['flag'] = df['result'].map(d_ranking)\n",
    "\n",
    "drop_list = ['result', 'rank3', 'rank4', '3ftime', 'time']\n",
    "df = df.drop(drop_list, axis=1)\n",
    "\n",
    "df['odds_hi'] = (df['odds'] / df['pop'])\n",
    "df['re_odds_hi'] = (df['pre_odds'] / df['pre_pop'])\n",
    "df['odds_hi*2'] = df['odds_hi'] ** 2\n",
    "df['re_odds_hi*2'] = df['re_odds_hi'] ** 2\n",
    "df['re_3_to_4time'] = (df['pre_rank4'] - df['pre_rank3'])\n",
    "df['re_3_to_4time_hi*2'] = (df['pre_rank4'] / df['pre_rank3']) ** 2\n",
    "df['father_3f_to_my'] = (df['father_3ftime'] - df['pre_3ftime'])\n",
    "df['fathertype_3f_to_my'] = (df['fathertype_3ftime'] - df['pre_3ftime'])\n",
    "df['re_pop_now_pop'] = (df['pre_pop'] - df['pop'])\n",
    "df['re_odds_now_odds'] = (df['pre_odds'] - df['odds'])\n",
    "df['re_result_to_pop'] = (df['pre_result'] - df['pre_pop'])\n",
    "\n",
    "feature_list = ['odds_hi', 're_odds_hi', 'odds_hi*2', 're_odds_hi*2', 're_3_to_4time', 're_3_to_4time_hi*2',\n",
    "                        'father_3f_to_my', 'fathertype_3f_to_my', 're_pop_now_pop', 're_odds_now_odds',\n",
    "                        're_result_to_pop']\n",
    "for feature in feature_list:\n",
    "    df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)\n",
    "    df[feature] = df[feature].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678dc956-61f7-4f4f-bc4a-05c0223992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBMでの実装\n",
    "cat_cols = ['place', 'class', 'turf', 'distance', 'weather', 'condition', 'sex', 'father', 'mother',\n",
    "                    'fathertype', 'fathermon', 'legtype', 'jocky', 'trainer', 'father_legtype']\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[c])\n",
    "    df[c] = le.transform(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7d0f0-0d6b-42b4-8e36-19a5787d59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days'] = pd.to_datetime(df['days'])\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "df_pred = df[df['days'] >= datetime(2021, 11, 7)]\n",
    "df_pred_droped = df_pred.drop(['flag', 'days', 'horsename', 'raceid', 'odds', 'pop'], axis=1)\n",
    "\n",
    "df = df[df['days'] < datetime(2021, 11, 7)]\n",
    "\n",
    "train_x = df.drop(['flag', 'days', 'horsename', 'raceid', 'odds', 'pop'], axis=1)\n",
    "train_y = df['flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y,\n",
    "                                                    stratify=train_y,\n",
    "                                                    random_state=0, test_size=0.3, shuffle=True)\n",
    "\n",
    "cat_cols = ['place', 'class', 'turf', 'distance', 'weather', 'condition', 'sex', 'father', 'mother',\n",
    "            'fathertype', 'fathermon', 'legtype', 'jocky', 'trainer', 'father_legtype']\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=cat_cols)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, categorical_feature=cat_cols)\n",
    "\n",
    "params = {\n",
    "    'task': 'predict',\n",
    "    'objective': 'binary',\n",
    "    'verbosity': -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    categorical_feature=cat_cols,\n",
    "    valid_sets=lgb_eval,\n",
    "    num_boost_round=100,\n",
    "    early_stopping_rounds=20,\n",
    ")\n",
    "best_params = model.params\n",
    "\n",
    "model = lgb.train(\n",
    "    best_params,\n",
    "    lgb_train,\n",
    "    categorical_feature=cat_cols,\n",
    "    valid_sets=lgb_eval,\n",
    "    num_boost_round=100,  # 100\n",
    "    early_stopping_rounds=20,  # 20\n",
    ")\n",
    "\n",
    "predict_proba = model.predict(df_pred_droped, num_iteration=model.best_iteration)\n",
    "\n",
    "gbm_predict = pd.DataFrame({\"raceid\": df_pred['raceid'],\n",
    "                        \"gbm_pred\": predict_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d714f-9edc-4e0a-b9e5-023c4a9e503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlowでの実装\n",
    "scaler = StandardScaler()\n",
    "sc = scaler.fit(df[num_data])\n",
    "\n",
    "scalered_df = pd.DataFrame(sc.transform(df[num_data]), columns=num_data, index=df.index)\n",
    "df.update(scalered_df)\n",
    "\n",
    "# ここからTensorFlow\n",
    "feature_columns = []\n",
    "\n",
    "num_data = datalist.num_datas\n",
    "\n",
    "for header in num_data:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "horsenum = feature_column.numeric_column('horsenum')\n",
    "horsenum_buckets = feature_column.bucketized_column(horsenum, [2, 4, 6, 8, 10, 12, 14, 16, 18])\n",
    "feature_columns.append(horsenum_buckets)\n",
    "\n",
    "cat_data = ['place', 'class', 'turf', 'weather', 'condition', 'sex', 'father', 'mother', 'fathermon',\n",
    "            'fathertype', 'legtype', 'jocky', 'trainer']\n",
    "\n",
    "for cat in cat_data:\n",
    "    category = feature_column.categorical_column_with_vocabulary_list(cat, list(df[cat].unique()))\n",
    "    feature_columns.append(feature_column.embedding_column(category, dimension=8))\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779ec7b-661e-42a6-ab2c-48ae9dab56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('flag')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "df['days'] = pd.to_datetime(df['days'])\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "df_pred = df[df['days'] >= datetime(2021, 11, 7)]\n",
    "df_pred_droped = df_pred.drop(['flag', 'days', 'horsename', 'raceid', 'odds', 'pop'], axis=1)\n",
    "\n",
    "df = df[df['days'] < datetime(2021, 11, 7)]\n",
    "df = df.drop(['days', 'horsename', 'raceid', 'odds', 'pop'], axis=1)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = self.df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = self.df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = self.df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "pred_ds = tf.data.Dataset.from_tensor_slices(dict(df_pred_droped))\n",
    "pred_ds = pred_ds.batch(batch_size=batch_size)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_layer,\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=5)\n",
    "\n",
    "# loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "predictions = model.predict(pred_ds)\n",
    "predict = [i for i in predictions]\n",
    "\n",
    "d = {\n",
    "    \"raceid\": df_pred['raceid'],\n",
    "    \"tf_pred\": predict\n",
    "}\n",
    "\n",
    "tf_predict = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23102cbb-289c-44cc-be90-65d310300f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均をとりフラグの作成\n",
    "\n",
    "df_pred = main_df[main_df['days'] >= datetime(2021, 11, 7)]\n",
    "\n",
    "df = pd.merge(gbm_predict, tf_predict, on='raceid', how='left')\n",
    "\n",
    "# gbm_pred, tf_pred\n",
    "df['new_mark_flag'] = '×'\n",
    "df['new_flag'] = 0\n",
    "\n",
    "# # 0.5が1個以上のフラグ作成。〇\n",
    "df['new_mark_flag'].mask((df['gbm_pred'] >= 0.5) | (df['tf_pred'] >= 0.5), '〇', inplace=True)\n",
    "\n",
    "# 0.5が2個以上のフラグ作成。◎\n",
    "df['new_mark_flag'].mask((df['gbm_pred'] >= 0.5) & (df['tf_pred'] >= 0.5), '◎', inplace=True)\n",
    "\n",
    "# 0.5以上をフラグ追加\n",
    "df['new_flag'].mask(((df['gbm_pred'] * 0.5) + (df['tf_pred'] * 0.5)) >= 0.5, 1, inplace=True)\n",
    "\n",
    "df = pd.merge(df_pred, df, on='raceid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b65b973-7a1f-4c3b-898e-95a654cddcd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2465742824.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    loss: 0.2861 - accuracy: 0.8771\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6974e4f-a5fb-436d-81bd-802b10d53822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
